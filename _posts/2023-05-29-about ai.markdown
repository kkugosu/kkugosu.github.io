---
layout: post
title: About AI
date: 2023-05-29 00:10:33 +0900
category: english
---

With advancement of ai, we can see plenty of conspicuous model these days which is usually comes from GAN and RL both of which are active learning model.
<br/>
<br/>
There has been numerous elaboration to find out like "what is ai", "how can we build most plausable ai" and "what is intelligence" and lots of paper successfully gave us deep inspiration about fundamental implementation of ai. Even though there has been enourmous pregress in AI, we still can't explain why we have curiousity and still failed to make how to make model to learn minimizing PAC-bayes bound(i will explain about that in the following paragraphs).
<br/>
<br/>
I gonna write about the most fundamental and important concepts we encounter when we first learn about AI which terms are namely bias and variance. actually these two concept imply opposite things. Bias means assumption and variance means non-assumption that is like even though we learn something which works simillar as preliminary learned, we should not assume that will works in a same way.
<br/>
<br/>
However, The matter at hand is, we have to strike a balance between two concept. For example, when you take the ramen and their color was red, you may think every red food is spicy if you are really biasd. However you have to descern in more specific way in the real life and take that as depending on the context. In the other hand, you should have bias to some extent. when you learned something from specific situation, you should be able to adapt that principle for another situation as well.
<br/>
<br/>
The representative theory which discuss about this is called PAC(as I referred before) and we usually employ PAC-bayes bounds in academic paper. As a matter of course, there has been attempts which aim to make model to autonomously strike a balance by themselves which is called autoML.
There are many different kinds of techniques which is included in autoML framework called HPO(hyper parameter optimization), NAS(neural architecture search), meta learning and more.
<br/>
<br/>
As it is still developing area, there are still many kinds of PAC-Bayes bounds. pac-bayes bound is kind of upper bound of error rate. so if you lower the pac bayes bounds, that means you successfully found the appropriate point of balance between bias and variance.
but a common characteristic among them is that the bound decreases as 1. estimated risk discreases, 2. the Kullback-Leibler divergence (KLD) between the posterior and prior distributions decreases or 3. the number of data for learning increases. (maybe it will be difficult to understand)
<br/>
<br/>
HPO and NAS is method of finding appropriate hypothetical space which is corresponding to 2 and meta learning is method of increasing number of data for learning which is corresponding to 3.
<br/>
<br/>
<br/>
korean translation for my friends who wanna learn korean
<br/>
<br/>
<br/>
AI가 발전함에따라 우리는 많은 신기한 모델들을 접할수 있게 되었고 그것들은 주로 RL이나 GAN같은 active learning 계열들입니다.
<br/>
<br/>
ai 란 무엇인가, 어떻게 해야 가장 그럴듯한 ai를 만들수있는가, 지능은 무엇인가 와 같은 질문에 대한 답을 찾기위한 많은 노력이 있어왔고, 많은 논문들이 우리에게 ai의 근본적인 작동방식에 대해 성공적으로 영감을 주었습니다. 하지만 이렇게 많은 성장에도 불구하고 우리는 여전히 우리가 왜 호기심을 가지고 있는지 설명하지 못하였고, PAC-bayes bound를 최소화시키지 못했습니다.
<br/>
<br/>
저는 우리가 ai를 처음배울때 접하는, ai에 있어서 가장 중요하고 근본적인 개념에 대해 글을 쓸것이며 그것들은 bias와 variance라고 불립니다. 실제로 이 두개념은 상반되는것을 나타냅니다. bias는 가정을 뜻하며 variance는 가정하지 않는것을 뜻합니다 즉 그것은 우리가 이전에 배운것들과 비슷해 보이는것을 배우더라도 그것들이 같은방식으로 동작할것이라고 가정하지 않는것입니다.
<br/>
<br/>
근데 문제는, 우리는 두 개념간의 균형을 유지해야 한다는것입니다. 예를들어서 만약 당신이 굉장히 biasd 되었다면, 당신이 라면을 먹고 그것의 색깔이 빨갈때, 모든 빨간음식이 매울것이라고 생각할것입니다. 그런데 당신은 실제의 삶에서 더더욱 세세하게 분별하고 상황에 따라서 다르게 받아들여야 합니다. 한편으로, 당신은 어느정도의 편견은 가지고 있어야합니다. 특정상황에서 어떤것을 배울때 이것을 다른 상황에서도 비슷하게 써먹을줄 알아야 합니다.
<br/>
<br/>
이것에 대하여 논하는 대표적인 이론이 PAC라고하며, 보통 논문에서 PAC-bayes bounds를 많이 활용합니다. 당연하게도 모델들이 스스로 균형을 맞추게 만들기 위한 많은 노력들이 있었고, 그것을 autoML이라고 합니다. automl은 HPO, NAS, meta learning등등 많은 종류가 있습니다.
<br/>
<br/>
이게 여전히 발전중인 학문이라서, 여전히 많은 종류의 PAC-bayes bound가 존재합니다. PAC-bayes bound는 오차율의 상한선 같은것입니다. 그래서 만약 당신이 pac bayes bound를 낮춘다면, 그것은 당신이 bias와 variance사이의 적절한 균형을 찾았다는 뜻입니다. 하지만 그들 사이의 bound를 낮추는 공통점은 1. 평가오차가 떨어지는것, 2. 사후분포와 사전분포의 kld가 낮아지는것. 3. 학습하는데 쓰인 데이터가 늘어나는것 입니다.
<br/>
<br/>
hpo 와 nas는 2번에 해당하는 적절한 가설공간을 찾는 방법이며, meta learning은 3번에 해당하는, 데이터를 늘리는 방법입니다.